{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import polars as pl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import shap\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('/Users/ignat/Desktop/Projects/dodo promocodes')\n",
    "def prepare_dataset():\n",
    "    df = pd.read_csv(root / 'data/train_target.csv')\n",
    "    orders = pd.read_csv(root / 'data/orders.csv')\n",
    "    events = pd.read_csv(root / 'data/mobile_events.csv')\n",
    "    promocodes = pd.read_csv(root / 'data/clients_promo_october.csv')\n",
    "\n",
    "    df.drop(['LocalBeginDate', 'LocalEndDate'], axis=1, inplace=True)\n",
    "\n",
    "    orders['SaleDate'] = pd.to_datetime(orders['SaleDate'])\n",
    "    orders['Date'] = pd.to_datetime(orders['Date'])\n",
    "\n",
    "    events['Timestamp'] = pd.to_datetime(events['Timestamp'])\n",
    "    promocodes['LocalBeginDate'] = pd.to_datetime(promocodes['LocalBeginDate'])\n",
    "    promocodes['LocalEndDate'] = pd.to_datetime(promocodes['LocalEndDate'])\n",
    "    events['Timestamp'] = events['Timestamp'].dt.floor('s')\n",
    "    \n",
    "    num_promocodes = promocodes.groupby('ClientUUId').agg({'Id': 'count'}).reset_index().rename(columns={'Id': 'num_promocodes'})\n",
    "    df_october = orders[orders.Date.dt.month == 10]\n",
    "    test_1 = df_october.groupby(['ClientUUId', 'OrderUUId']).agg(\n",
    "        {'apply_promo': 'first'}\n",
    "    ).reset_index().drop('OrderUUId', axis=1).groupby('ClientUUId') \\\n",
    "    .agg({'apply_promo': 'sum'}).reset_index().rename(columns={'apply_promo': 'apply_promo_used_last_month'})\n",
    "\n",
    "    test_1 = test_1.merge(num_promocodes, on='ClientUUId')\n",
    "    test_1['ratio_apply_promo_to_num_promocodes'] = test_1['apply_promo_used_last_month'] / test_1['num_promocodes']\n",
    "    test_1.drop(columns=['apply_promo_used_last_month', 'num_promocodes'], inplace=True)\n",
    "    df = df.merge(test_1, on='ClientUUId', how='left')\n",
    "    events['hour'] = events['Timestamp'].dt.hour\n",
    "    events['weekday'] = events['Timestamp'].dt.weekday\n",
    "    df_events = pl.from_pandas(events).group_by('ClientUUId').agg(\n",
    "        pl.col('VisitToken').n_unique().alias('VisitToken_n_unique'),\n",
    "        pl.col('hour').median().alias('hour_events_median'),\n",
    "        pl.col('Platform').first(),\n",
    "        pl.col('VisitToken').filter(pl.col('weekday') > 3).n_unique().alias('num_visits_from_thursday'),\n",
    "        pl.col('VisitToken').filter(pl.col('weekday') < 4).n_unique().alias('num_visits_not_in_promo'),\n",
    "    ).to_pandas()\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, dtype=np.int8)\n",
    "    ohe.fit(events[['EventName']])\n",
    "\n",
    "    new_features = pd.DataFrame(ohe.transform(events[['EventName']]), columns=ohe.get_feature_names_out())\n",
    "    events = pd.concat([events, new_features], axis=1)\n",
    "    events = pl.from_pandas(events)\n",
    "    df_events2 = events.group_by('ClientUUId').agg(\n",
    "        *[pl.col(col).mean().alias(f'{col}_mean') for col in ohe.get_feature_names_out()],\n",
    "        *[pl.col(col).sum().alias(f'{col}_sum') for col in ohe.get_feature_names_out()],\n",
    "        ((pl.datetime(2023, 11, 2, time_unit='ns', time_zone='UTC') - pl.col('Timestamp').last()).dt.total_seconds() / 3600).alias('last_online')\n",
    "    )\n",
    "    \n",
    "    df_events = df_events.merge(df_events2.to_pandas(), on='ClientUUId', how='left')\n",
    "    events = events.to_pandas()\n",
    "    \n",
    "    \n",
    "    df_events['Platform'] = df_events['Platform'].map({'ios': 1, 'android': 0})\n",
    "    df = df.merge(df_events, on='ClientUUId', how='left')\n",
    "    \n",
    "    id_ohe = OneHotEncoder(sparse_output=False, dtype=np.int8)\n",
    "    id_features = pd.DataFrame(id_ohe.fit_transform(promocodes[['Id']]), columns=id_ohe.get_feature_names_out())\n",
    "    promocodes = pd.concat([promocodes, id_features], axis=1)\n",
    "    promocodes['promocode_duration'] = ((promocodes['LocalEndDate'] - promocodes['LocalBeginDate']).dt.total_seconds() / 3600)\n",
    "    promocodes['OrderType'] = promocodes['OrderType'].map({'2,3': 0, '1,2,3': 1})\n",
    "    promocodes['Discount_percenteges'] = np.where(promocodes['Discount'] <= 100, promocodes['Discount'], 0)\n",
    "    promocodes['Discount_usual'] = np.where(promocodes['Discount'] > 100, promocodes['Discount'], 0)\n",
    "    promocodes = pl.from_pandas(promocodes)\n",
    "    promocodes = promocodes.group_by('ClientUUId').agg(\n",
    "        pl.col('Id').count().alias('num_promocodes'),\n",
    "        *[pl.col(col).mean().alias(f'{col}_mean') for col  in id_ohe.get_feature_names_out()],\n",
    "        *[pl.col(col).sum().alias(f'{col}_sum') for col  in id_ohe.get_feature_names_out()],\n",
    "        pl.col('OrderPrice').max().alias('OrderPrice_max'),\n",
    "        pl.col('OrderPrice').min().alias('OrderPrice_min'),\n",
    "        pl.col('OrderPrice').median().alias('OrderPrice_median'),\n",
    "        pl.col('Discount').max().alias('Discount_max'),\n",
    "        pl.col('Discount').min().alias('Discount_min'),\n",
    "        pl.col('Discount').median().alias('Discount_median'),\n",
    "    )\n",
    "    promocodes = promocodes.to_pandas()\n",
    "    df = df.merge(promocodes, on='ClientUUId', how='left')\n",
    "    \n",
    "    orders['in_restaurant'] = orders['addressId'].isnull().astype(np.int8)\n",
    "    \n",
    "    df_orders = orders.groupby('ClientUUId').agg({'NewClient': 'max', 'Date': ['max', 'min'], \n",
    "                                  'ClientOrderNumber': ['max', 'min'],\n",
    "                                  }).reset_index()\n",
    "\n",
    "    df_orders.columns = ['_'.join(col).strip() for col in df_orders.columns.values]\n",
    "    df_orders.rename(columns={'ClientUUId_': 'ClientUUId'}, inplace=True)\n",
    "\n",
    "    df_orders['orders_in_this_year'] = df_orders['ClientOrderNumber_max'] - df_orders['ClientOrderNumber_min']\n",
    "    df_orders['last_order_was'] = (datetime(2023, 11, 1) - df_orders['Date_max']).dt.days \n",
    "\n",
    "    orders.drop(columns=['NewClient'], inplace=True, errors='ignore')\n",
    "\n",
    "    orders['discount_for_product'] = orders['MenuPrice'] - orders['ProductTotalPrice']\n",
    "    \n",
    "\n",
    "    enc_category = OneHotEncoder(sparse_output=False)\n",
    "    enc_category.fit(orders[['CategoryId']])\n",
    "    categories_orders  = pd.DataFrame(enc_category.transform(orders[['CategoryId']]), columns=enc_category.get_feature_names_out())\n",
    "\n",
    "    orders = pd.concat([orders, categories_orders], axis=1) \n",
    "    orders = pl.from_pandas(orders)\n",
    "    orders_agg = orders.group_by(['ClientUUId', 'OrderUUId']).agg(\n",
    "        pl.col('in_restaurant').first(),\n",
    "        pl.col('OrderTotalPrice').first(),\n",
    "        pl.col('OrderPaymentType').first(),\n",
    "        pl.col('OrderType').first(),\n",
    "        pl.col('OrderState').first(),\n",
    "        pl.col('ClientOrderNumber').first(),\n",
    "        pl.col('apply_promo').first(),\n",
    "        pl.col('Date').first(),\n",
    "        pl.col('addressId').first(),\n",
    "        pl.col('deliverySectorId').first(),\n",
    "        pl.col('SaleDate').first(),\n",
    "        pl.col('UnitUUId').first(),\n",
    "        pl.col('ProductUUId').count().alias('ProductUUId_count'),\n",
    "        pl.col('ProductUUId').unique().count().alias('ProductUUId_unique_count'),\n",
    "        pl.col('discount_for_product').mean().alias('discount_for_product_mean'),\n",
    "        pl.col('discount_for_product').sum().alias('discount_for_product_sum'),\n",
    "        pl.col('CategoryId').mode().get(0).alias('CategoryId_mode'),\n",
    "        pl.col('MenuPrice').max().alias('MenuPrice_max'),\n",
    "        pl.col('MenuPrice').min().alias('MenuPrice_min'),\n",
    "        pl.col('MenuPrice').median().alias('MenuPrice_median'),\n",
    "        pl.col('ProductTotalPrice').max().alias('ProductTotalPrice_max'),\n",
    "        pl.col('ProductTotalPrice').min().alias('ProductTotalPrice_min'),\n",
    "        pl.col('ProductTotalPrice').median().alias('ProductTotalPrice_median'),\n",
    "        pl.col('ProductTotalPrice').sum().alias('ProductTotalPrice_sum'),\n",
    "        pl.col('ProductTotalPrice').mean().alias('ProductTotalPrice_mean'),\n",
    "        *[pl.col(col).first() for col in enc_category.get_feature_names_out()],\n",
    "        \n",
    "    )\n",
    "    orders_agg = orders_agg.sort(by=['ClientUUId', 'ClientOrderNumber'])\n",
    "\n",
    "    orders_agg = orders_agg.group_by('ClientUUId').agg(\n",
    "        *[pl.col(col).mean().alias(f'{col}_mean') for col in enc_category.get_feature_names_out()],\n",
    "        *[pl.col(col).sum().alias(f'{col}_sum') for col in enc_category.get_feature_names_out()], \n",
    "        *[func('OrderTotalPrice').alias(f'OrderTotalPrice_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last]],\n",
    "        *[func('apply_promo').alias(f'apply_promo_{func.__name__}') for func in [pl.sum, pl.mean, pl.last]],\n",
    "        pl.last('ClientOrderNumber').alias('ClientOrderNumber_last'),\n",
    "        pl.col('deliverySectorId').unique().count().alias(f'deliverySectorId_unq_count'),\n",
    "        *[func('MenuPrice_max').alias(f'MenuPrice_max_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('MenuPrice_min').alias(f'MenuPrice_min_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('MenuPrice_median').alias(f'MenuPrice_median_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductTotalPrice_max').alias(f'ProductTotalPrice_max_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductTotalPrice_min').alias(f'ProductTotalPrice_min_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductTotalPrice_median').alias(f'ProductTotalPrice_median_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductUUId_unique_count').alias(f'ProductUUId_unique_count_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        pl.col('apply_promo').filter(pl.col('Date').dt.month() == 10).mean().alias('apply_promo_mean_last_month'),\n",
    "        pl.col('apply_promo').filter(pl.col('Date').dt.month() == 10).sum().alias('apply_promo_sum_last_month'),\n",
    "        pl.col('in_restaurant').filter(pl.col('Date').dt.month() == 10).mean().alias('in_restaurant_mean_last_month'),\n",
    "        pl.col('in_restaurant').filter(pl.col('Date').dt.month() == 10).sum().alias('in_restaurant_sum_last_month'),\n",
    "        pl.col('in_restaurant').mean().alias('in_restaurant_mean'),\n",
    "        pl.col('in_restaurant').sum().alias('in_restaurant_sum'),\n",
    "        *[func('ProductTotalPrice_sum').alias(f'ProductTotalPrice_sum_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.sum]],\n",
    "        *[func('ProductTotalPrice_mean').alias(f'ProductTotalPrice_mean_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.sum]],\n",
    "        pl.col('OrderTotalPrice').filter(pl.col('Date').dt.month() == 10).mean().alias('OrderTotalPrice_mean_last_month'),\n",
    "        pl.col('OrderTotalPrice').filter(pl.col('Date').dt.month() == 10).sum().alias('OrderTotalPrice_sum_last_month'),\n",
    "    )\n",
    "    \n",
    "    df_orders = df_orders.merge(orders_agg.to_pandas(), on='ClientUUId', how='left')\n",
    "    df = df.merge(df_orders, on='ClientUUId', how='left')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['groups'] = le.fit_transform(df[['ClientUUId']])\n",
    "    test = pd.read_csv(root / 'data/test-2.csv')\n",
    "    \n",
    "    promocodes_copy = pd.read_csv(root / 'data/clients_promo_october.csv')\n",
    "    promocodes_copy['LocalBeginDate'] = pd.to_datetime(promocodes_copy['LocalBeginDate']).dt.tz_convert(None)\n",
    "    promocodes_copy['LocalEndDate'] = pd.to_datetime(promocodes_copy['LocalEndDate']).dt.tz_convert(None)\n",
    "    orders_agg = orders.group_by(['ClientUUId', 'OrderUUId']).agg(\n",
    "        pl.col('apply_promo').first(),\n",
    "        pl.col('Date').first(),\n",
    "        pl.col('OrderTotalPrice').first(),\n",
    "        pl.col('OrderType').first(),\n",
    "    ).filter((pl.col('apply_promo') == 1) & (pl.col('Date').dt.month() == 10))\n",
    "    orders_agg = orders_agg.join(pl.from_pandas(promocodes_copy), on='ClientUUId', how='left')\n",
    "    orders_agg = orders_agg.filter((pl.col('OrderTotalPrice')>=pl.col('OrderPrice')) & (pl.col('LocalBeginDate') <= pl.col('Date')) \n",
    "                    & (pl.col('Date') <= pl.col('LocalEndDate')) & (pl.col('OrderType_right').str.find(pl.col('OrderType')) > -1)).to_pandas()\n",
    "\n",
    "    idx = orders_agg.groupby(['ClientUUId', 'Date'])['OrderPrice'].idxmax()\n",
    "    orders_agg = orders_agg.loc[idx].sort_values(by='ClientUUId')\n",
    "    orders_agg.sort_values(by=['ClientUUId', 'Date'])\n",
    "    orders_agg = pl.from_pandas(orders_agg)\n",
    "    orders_agg = orders_agg.group_by(['ClientUUId']).agg(\n",
    "        pl.col('OrderTotalPrice').last().alias('OrderTotalPrice_last_fp'),\n",
    "        pl.col('OrderType').last().alias('OrderType_last_fp'),\n",
    "        pl.col('Id').last().alias('Id_last_fp'),\n",
    "        pl.col('OrderPrice').last().alias('OrderPrice_last_fp'),\n",
    "        pl.col('Discount').last().alias('Discount_last_fp'),\n",
    "    ).to_pandas()\n",
    "    df = df.merge(orders_agg , on='ClientUUId', how='left')\n",
    "    columns = ['OrderTotalPrice_last_fp', 'OrderType_last_fp', 'Id_last_fp', 'OrderPrice_last_fp', 'Discount_last_fp']\n",
    "    df[columns] = df[columns].fillna(-1)\n",
    "    promocodes_equality = df[df.apply(lambda row: row['OrderType'].find(str(row['OrderType_last_fp'])) > -1 \n",
    "                                      and row['Id'] == row['Id_last_fp'], axis=1)][['ClientUUId', 'Id', 'OrderType']]\n",
    "    promocodes_equality['promocodes_equality'] = 1\n",
    "    df = df.merge(promocodes_equality, on=['ClientUUId', 'Id', 'OrderType'], how='left')\n",
    "    df['promocodes_equality'] = df['promocodes_equality'].fillna(0)\n",
    "\n",
    "    test = test.merge(orders_agg , on='ClientUUId', how='left')\n",
    "    columns = ['OrderTotalPrice_last_fp', 'OrderType_last_fp', 'Id_last_fp', 'OrderPrice_last_fp', 'Discount_last_fp']\n",
    "    test[columns] = test[columns].fillna(-1)\n",
    "    test = test.merge(promocodes_equality, on=['ClientUUId', 'Id', 'OrderType'], how='left')\n",
    "    test['promocodes_equality'] = test['promocodes_equality'].fillna(0)\n",
    "    \n",
    "    test = test.merge(df_orders, on='ClientUUId', how='left')\n",
    "    test = test.merge(df_events, on='ClientUUId', how='left')\n",
    "    test = test.merge(promocodes, on='ClientUUId', how='left')\n",
    "    test = test.merge(test_1, on='ClientUUId', how='left')\n",
    "    \n",
    "    df.drop(columns, inplace=True, axis=1)\n",
    "    test.drop(columns, inplace=True, axis=1)\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse_output=False, dtype=np.int8)\n",
    "    ohe.fit(df[['Id']])\n",
    "\n",
    "    new_features = pd.DataFrame(ohe.transform(df[['Id']]), columns=ohe.get_feature_names_out())\n",
    "    df = pd.concat([df, new_features], axis=1)\n",
    "    \n",
    "    new_features = pd.DataFrame(ohe.transform(test[['Id']]), columns=ohe.get_feature_names_out())\n",
    "    test = pd.concat([test, new_features], axis=1)\n",
    "    \n",
    "    test['Is_id_7_mean'] = test['Id_7_mean'] * test['Id_7']\n",
    "    test['Is_id_7_sum'] = test['Id_7_sum'] * test['Id_7']\n",
    "\n",
    "    test['Is_id_6_mean'] = test['Id_6_mean'] * test['Id_6']\n",
    "    test['Is_id_6_sum'] = test['Id_6_sum'] * test['Id_6']\n",
    "\n",
    "    test['Is_id_5_mean'] = test['Id_5_mean'] * test['Id_5']\n",
    "    test['Is_id_5_sum'] = test['Id_5_sum'] * test['Id_5']\n",
    "\n",
    "    df['Is_id_7_mean'] = df['Id_7_mean'] * df['Id_7']\n",
    "    df['Is_id_7_sum'] = df['Id_7_sum'] * df['Id_7']\n",
    "\n",
    "    df['Is_id_6_mean'] = df['Id_6_mean'] * df['Id_6']\n",
    "    df['Is_id_6_sum'] = df['Id_6_sum'] * df['Id_6']\n",
    "\n",
    "    df['Is_id_5_mean'] = df['Id_5_mean'] * df['Id_5']\n",
    "    df['Is_id_5_sum'] = df['Id_5_sum'] * df['Id_5']\n",
    "    return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df, test = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27741, 135),\n",
       " ['ClientOrderNumber_min',\n",
       "  'CategoryId_2_sum',\n",
       "  'CategoryId_3_sum',\n",
       "  'CategoryId_4_sum',\n",
       "  'CategoryId_5_sum',\n",
       "  'CategoryId_6_sum',\n",
       "  'CategoryId_7_sum',\n",
       "  'apply_promo_mean',\n",
       "  'ClientOrderNumber_last',\n",
       "  'in_restaurant_sum',\n",
       "  'ProductTotalPrice_sum_max',\n",
       "  'ProductTotalPrice_sum_min',\n",
       "  'ProductTotalPrice_sum_median'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_high_correlation_features(df):\n",
    "    corr_matrix = df.select_dtypes([float, int]).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "    return to_drop\n",
    "to_drop = find_high_correlation_features(df)\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "test.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "df.shape, to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_for_model_1 = df.dropna(axis=0).index\n",
    "X = df.iloc[index_for_model_1].select_dtypes([float, int]).drop(columns=['apply_promo', 'groups'])\n",
    "y = df.iloc[index_for_model_1]['apply_promo']\n",
    "groups = df.iloc[index_for_model_1]['groups']\n",
    "result_df = pd.DataFrame({'score': 0.0, 'ClientUUId': df['ClientUUId'], 'OrderType' : df['OrderType'], 'target': df['apply_promo']\n",
    "                          }, index=df.iloc[index_for_model_1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "[0]\ttrain-auc:0.68836\teval-auc:0.72625\n",
      "[50]\ttrain-auc:0.82253\teval-auc:0.75794\n",
      "[100]\ttrain-auc:0.86073\teval-auc:0.76082\n",
      "[110]\ttrain-auc:0.86409\teval-auc:0.76000\n",
      "Fold 2:\n",
      "[0]\ttrain-auc:0.64673\teval-auc:0.63596\n",
      "[50]\ttrain-auc:0.82435\teval-auc:0.78086\n",
      "[100]\ttrain-auc:0.85148\teval-auc:0.78872\n",
      "[150]\ttrain-auc:0.86610\teval-auc:0.79191\n",
      "[200]\ttrain-auc:0.87207\teval-auc:0.79600\n",
      "[250]\ttrain-auc:0.87546\teval-auc:0.79951\n",
      "[300]\ttrain-auc:0.87749\teval-auc:0.80039\n",
      "[350]\ttrain-auc:0.87921\teval-auc:0.80126\n",
      "[400]\ttrain-auc:0.88205\teval-auc:0.80286\n",
      "[450]\ttrain-auc:0.88467\teval-auc:0.80435\n",
      "[500]\ttrain-auc:0.88599\teval-auc:0.80453\n",
      "[550]\ttrain-auc:0.88775\teval-auc:0.80656\n",
      "[600]\ttrain-auc:0.88856\teval-auc:0.80732\n",
      "[650]\ttrain-auc:0.88959\teval-auc:0.80767\n",
      "[700]\ttrain-auc:0.89067\teval-auc:0.80829\n",
      "[750]\ttrain-auc:0.89255\teval-auc:0.80859\n",
      "[794]\ttrain-auc:0.89298\teval-auc:0.80880\n",
      "Fold 3:\n",
      "[0]\ttrain-auc:0.71942\teval-auc:0.63254\n",
      "[50]\ttrain-auc:0.82505\teval-auc:0.76671\n",
      "[100]\ttrain-auc:0.85515\teval-auc:0.77017\n",
      "[150]\ttrain-auc:0.86878\teval-auc:0.77502\n",
      "[200]\ttrain-auc:0.87705\teval-auc:0.77574\n",
      "[250]\ttrain-auc:0.88050\teval-auc:0.77489\n",
      "[283]\ttrain-auc:0.88222\teval-auc:0.77437\n",
      "Fold 4:\n",
      "[0]\ttrain-auc:0.69612\teval-auc:0.67418\n",
      "[50]\ttrain-auc:0.82295\teval-auc:0.70754\n",
      "[100]\ttrain-auc:0.85727\teval-auc:0.71968\n",
      "[150]\ttrain-auc:0.87069\teval-auc:0.71745\n",
      "[163]\ttrain-auc:0.87400\teval-auc:0.71810\n",
      "Fold 5:\n",
      "[0]\ttrain-auc:0.71477\teval-auc:0.65435\n",
      "[50]\ttrain-auc:0.82012\teval-auc:0.73737\n",
      "[100]\ttrain-auc:0.84896\teval-auc:0.76299\n",
      "[150]\ttrain-auc:0.85953\teval-auc:0.77076\n",
      "[200]\ttrain-auc:0.86974\teval-auc:0.77988\n",
      "[250]\ttrain-auc:0.87609\teval-auc:0.78656\n",
      "[300]\ttrain-auc:0.87870\teval-auc:0.78569\n",
      "[350]\ttrain-auc:0.88213\teval-auc:0.78774\n",
      "[400]\ttrain-auc:0.88459\teval-auc:0.78793\n",
      "[450]\ttrain-auc:0.88671\teval-auc:0.78862\n",
      "[462]\ttrain-auc:0.88816\teval-auc:0.78732\n",
      "Fold 6:\n",
      "[0]\ttrain-auc:0.68187\teval-auc:0.64867\n",
      "[50]\ttrain-auc:0.82531\teval-auc:0.75805\n",
      "[100]\ttrain-auc:0.85688\teval-auc:0.76217\n",
      "[150]\ttrain-auc:0.87491\teval-auc:0.76372\n",
      "[200]\ttrain-auc:0.88360\teval-auc:0.76940\n",
      "[250]\ttrain-auc:0.88709\teval-auc:0.76911\n",
      "[300]\ttrain-auc:0.88998\teval-auc:0.77150\n",
      "[350]\ttrain-auc:0.89158\teval-auc:0.77246\n",
      "[400]\ttrain-auc:0.89262\teval-auc:0.77289\n",
      "[450]\ttrain-auc:0.89346\teval-auc:0.77187\n",
      "[463]\ttrain-auc:0.89387\teval-auc:0.77150\n",
      "Fold 7:\n",
      "[0]\ttrain-auc:0.71832\teval-auc:0.69087\n",
      "[50]\ttrain-auc:0.82897\teval-auc:0.78266\n",
      "[100]\ttrain-auc:0.85454\teval-auc:0.79224\n",
      "[150]\ttrain-auc:0.86822\teval-auc:0.80177\n",
      "[200]\ttrain-auc:0.87560\teval-auc:0.80570\n",
      "[250]\ttrain-auc:0.87774\teval-auc:0.80858\n",
      "[300]\ttrain-auc:0.88312\teval-auc:0.80717\n",
      "[350]\ttrain-auc:0.88477\teval-auc:0.80668\n",
      "[352]\ttrain-auc:0.88477\teval-auc:0.80668\n",
      "Fold 8:\n",
      "[0]\ttrain-auc:0.70543\teval-auc:0.73347\n",
      "[50]\ttrain-auc:0.81818\teval-auc:0.84633\n",
      "[100]\ttrain-auc:0.84928\teval-auc:0.84549\n",
      "[130]\ttrain-auc:0.86309\teval-auc:0.84749\n",
      "Fold 9:\n",
      "[0]\ttrain-auc:0.73306\teval-auc:0.67625\n",
      "[50]\ttrain-auc:0.82321\teval-auc:0.74000\n",
      "[100]\ttrain-auc:0.85851\teval-auc:0.75649\n",
      "[150]\ttrain-auc:0.86852\teval-auc:0.75795\n",
      "[200]\ttrain-auc:0.87423\teval-auc:0.76027\n",
      "[250]\ttrain-auc:0.87976\teval-auc:0.76283\n",
      "[300]\ttrain-auc:0.88323\teval-auc:0.76727\n",
      "[350]\ttrain-auc:0.88611\teval-auc:0.77003\n",
      "[400]\ttrain-auc:0.88782\teval-auc:0.77208\n",
      "[450]\ttrain-auc:0.88914\teval-auc:0.77115\n",
      "[474]\ttrain-auc:0.88975\teval-auc:0.77156\n",
      "Fold 10:\n",
      "[0]\ttrain-auc:0.68157\teval-auc:0.67852\n",
      "[50]\ttrain-auc:0.82223\teval-auc:0.78299\n",
      "[100]\ttrain-auc:0.85384\teval-auc:0.78736\n",
      "[113]\ttrain-auc:0.86029\teval-auc:0.78819\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "num_iter = []\n",
    "skf = StratifiedGroupKFold(n_splits=10)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y, groups)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index])\n",
    "    dtest = xgb.DMatrix(X.iloc[test_index], label=y.iloc[test_index])\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"n_estimators\": 3000,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "        'max_depth': 5,\n",
    "        'colsample_bytree': 0.6489131779549984,\n",
    "        'colsample_bynode': 0.6009610046445969,\n",
    "        'colsample_bylevel': 0.7957273275573362,\n",
    "        'subsample': 0.6805161691122295,\n",
    "        'alpha': 8.371930284576981,\n",
    "        'lambda': 5.649133354153159,\n",
    "        'gamma': 7.103762168902399,\n",
    "        'min_child_weight': 4.101573431623777,\n",
    "        'max_delta_step': 1,\n",
    "        'sampling_method': 'uniform'\n",
    "    }\n",
    "\n",
    "    evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=90,\n",
    "        verbose_eval=50,\n",
    "    )\n",
    "\n",
    "    result_df.iloc[test_index, 0] = model.predict(dtest)\n",
    "    models.append(model)\n",
    "    num_iter.append(model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7820448943704266"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(result_df['target'].astype(float), result_df['score'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.69688\n",
      "[50]\ttrain-auc:0.82463\n",
      "[100]\ttrain-auc:0.85400\n",
      "[150]\ttrain-auc:0.87119\n",
      "[200]\ttrain-auc:0.87842\n",
      "[227]\ttrain-auc:0.88121\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"n_estimators\": 3000,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "        'max_depth': 5,\n",
    "        'colsample_bytree': 0.6489131779549984,\n",
    "        'colsample_bynode': 0.6009610046445969,\n",
    "        'colsample_bylevel': 0.7957273275573362,\n",
    "        'subsample': 0.6805161691122295,\n",
    "        'alpha': 8.371930284576981,\n",
    "        'lambda': 5.649133354153159,\n",
    "        'gamma': 7.103762168902399,\n",
    "        'min_child_weight': 4.101573431623777,\n",
    "        'max_delta_step': 1,\n",
    "        'sampling_method': 'uniform'\n",
    "}\n",
    "watchlist = [(dtrain, 'train')]\n",
    "meta_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=int(np.median(num_iter)),\n",
    "    evals=watchlist,\n",
    "    verbose_eval=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model without orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27741, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=1,thresh=18_000)\n",
    "\n",
    "X = df.select_dtypes([float, int]).drop(columns=['apply_promo', 'groups'])\n",
    "y = df['apply_promo']\n",
    "groups = df['groups']\n",
    "result_df2 = pd.DataFrame({'score': 0.0, 'ClientUUId': df['ClientUUId'], 'OrderType' : df['OrderType'], 'target': df['apply_promo']\n",
    "                          }, index=df.index)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "[0]\ttrain-auc:0.63550\teval-auc:0.60419\n",
      "[100]\ttrain-auc:0.77726\teval-auc:0.75021\n",
      "[200]\ttrain-auc:0.79472\teval-auc:0.75724\n",
      "[300]\ttrain-auc:0.79689\teval-auc:0.75824\n",
      "[400]\ttrain-auc:0.79827\teval-auc:0.75835\n",
      "[431]\ttrain-auc:0.79843\teval-auc:0.75856\n",
      "Fold 2:\n",
      "[0]\ttrain-auc:0.67530\teval-auc:0.65722\n",
      "[100]\ttrain-auc:0.79175\teval-auc:0.68705\n",
      "[200]\ttrain-auc:0.80467\teval-auc:0.68739\n",
      "[239]\ttrain-auc:0.80596\teval-auc:0.68865\n",
      "Fold 3:\n",
      "[0]\ttrain-auc:0.67240\teval-auc:0.69539\n",
      "[100]\ttrain-auc:0.78205\teval-auc:0.77130\n",
      "[200]\ttrain-auc:0.79606\teval-auc:0.77498\n",
      "[300]\ttrain-auc:0.79858\teval-auc:0.77810\n",
      "[400]\ttrain-auc:0.80073\teval-auc:0.77851\n",
      "[500]\ttrain-auc:0.80166\teval-auc:0.78033\n",
      "[592]\ttrain-auc:0.80197\teval-auc:0.78040\n",
      "Fold 4:\n",
      "[0]\ttrain-auc:0.68222\teval-auc:0.69391\n",
      "[100]\ttrain-auc:0.78747\teval-auc:0.74119\n",
      "[169]\ttrain-auc:0.79633\teval-auc:0.73838\n",
      "Fold 5:\n",
      "[0]\ttrain-auc:0.68213\teval-auc:0.62162\n",
      "[100]\ttrain-auc:0.79340\teval-auc:0.69747\n",
      "[200]\ttrain-auc:0.80541\teval-auc:0.70794\n",
      "[300]\ttrain-auc:0.80834\teval-auc:0.70757\n",
      "[362]\ttrain-auc:0.81035\teval-auc:0.70747\n",
      "Fold 6:\n",
      "[0]\ttrain-auc:0.70834\teval-auc:0.67194\n",
      "[100]\ttrain-auc:0.78532\teval-auc:0.76209\n",
      "[168]\ttrain-auc:0.79479\teval-auc:0.76702\n",
      "Fold 7:\n",
      "[0]\ttrain-auc:0.69570\teval-auc:0.62211\n",
      "[100]\ttrain-auc:0.79516\teval-auc:0.70370\n",
      "[200]\ttrain-auc:0.80329\teval-auc:0.70755\n",
      "[213]\ttrain-auc:0.80394\teval-auc:0.70711\n",
      "Fold 8:\n",
      "[0]\ttrain-auc:0.58574\teval-auc:0.68045\n",
      "[100]\ttrain-auc:0.78517\teval-auc:0.77995\n",
      "[113]\ttrain-auc:0.78969\teval-auc:0.78155\n",
      "Fold 9:\n",
      "[0]\ttrain-auc:0.66869\teval-auc:0.58298\n",
      "[91]\ttrain-auc:0.79668\teval-auc:0.63185\n",
      "Fold 10:\n",
      "[0]\ttrain-auc:0.66299\teval-auc:0.63403\n",
      "[100]\ttrain-auc:0.78796\teval-auc:0.73467\n",
      "[200]\ttrain-auc:0.80073\teval-auc:0.73701\n",
      "[300]\ttrain-auc:0.80714\teval-auc:0.73988\n",
      "[400]\ttrain-auc:0.80975\teval-auc:0.74182\n",
      "[500]\ttrain-auc:0.81071\teval-auc:0.74043\n",
      "[501]\ttrain-auc:0.81071\teval-auc:0.74043\n"
     ]
    }
   ],
   "source": [
    "models_2 = []\n",
    "num_iter_2 = []\n",
    "skf = StratifiedGroupKFold(n_splits=10)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y, groups)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index])\n",
    "    dtest = xgb.DMatrix(X.iloc[test_index], label=y.iloc[test_index])\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "        'max_depth': 4,\n",
    "        'colsample_bytree': 0.7055286391212441,\n",
    "        'colsample_bynode': 0.6915897982585256,\n",
    "        'colsample_bylevel': 0.7855187905031771,\n",
    "        'subsample': 0.7684073221158629,\n",
    "        'alpha': 5.250995916757242,\n",
    "        'lambda': 0.7316373694506648,\n",
    "        'gamma': 7.108685845995447,\n",
    "        'min_child_weight': 6.354648841169535,\n",
    "        'max_delta_step': 0,\n",
    "        # 'sampling_method': 'gradient_based'\n",
    " }\n",
    "        \n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=90,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    result_df2.iloc[test_index, 0] = model.predict(dtest)\n",
    "    models_2.append(model)\n",
    "    num_iter_2.append(model.best_iteration + 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=result_df2.columns)\n",
    "for row in result_df2.iterrows():\n",
    "    if row[0] in result_df.index:\n",
    "        result.loc[row[0], :] = result_df.loc[row[0]]\n",
    "    else:\n",
    "        result.loc[row[0], :] = result_df2.iloc[row[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7751348309124527"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(result['target'].astype(float), result['score'].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.68411\n",
      "[50]\ttrain-auc:0.76100\n",
      "[100]\ttrain-auc:0.79308\n",
      "[135]\ttrain-auc:0.79955\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X, label=y)\n",
    "params = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"verbosity\": 0,\n",
    "    'max_depth': 4,\n",
    "    'colsample_bytree': 0.7055286391212441,\n",
    "    'colsample_bynode': 0.6915897982585256,\n",
    "    'colsample_bylevel': 0.7855187905031771,\n",
    "    'subsample': 0.7684073221158629,\n",
    "    'alpha': 5.250995916757242,\n",
    "    'lambda': 0.7316373694506648,\n",
    "    'gamma': 7.108685845995447,\n",
    "    'min_child_weight': 6.354648841169535,\n",
    "    'max_delta_step': 0,\n",
    "}\n",
    "watchlist = [(dtrain, 'train')]\n",
    "meta_model_2 = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=int(np.median(num_iter_2)) - 90,\n",
    "    evals=watchlist,\n",
    "    verbose_eval=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.select_dtypes([float, int])\n",
    "test_result = pd.read_csv('data/test-2.csv')\n",
    "test_result['apply_promo'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6806/6806 [01:27<00:00, 77.49it/s] \n"
     ]
    }
   ],
   "source": [
    "final_scores = []\n",
    "\n",
    "for i in tqdm(range(len(test))):\n",
    "    scores = []\n",
    "    data = test.iloc[i]\n",
    "    if data.isnull().sum() > 40:\n",
    "        data = pd.DataFrame(data.to_frame().T)\n",
    "        data = data[models_2[0].feature_names]\n",
    "        data_dmatrix = xgb.DMatrix(data)\n",
    "        scores.append(meta_model_2.predict(data_dmatrix))\n",
    "        # for model in models_2:\n",
    "        #     scores.append(model.predict(data_dmatrix))\n",
    "    else:\n",
    "        data = pd.DataFrame(data.to_frame().T)\n",
    "        data = data[models[0].feature_names]\n",
    "        data_dmatrix = xgb.DMatrix(data)\n",
    "        scores.append(meta_model.predict(data_dmatrix))\n",
    "        # for model in models:\n",
    "        #     scores.append(model.predict(data_dmatrix))\n",
    "            \n",
    "    final_score = np.mean(scores)\n",
    "    final_scores.append(final_score)\n",
    "\n",
    "test_result.iloc[:, -1] = final_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClientUUId</th>\n",
       "      <th>Id</th>\n",
       "      <th>OrderType</th>\n",
       "      <th>LocalBeginDate</th>\n",
       "      <th>LocalEndDate</th>\n",
       "      <th>OrderPrice</th>\n",
       "      <th>Discount</th>\n",
       "      <th>apply_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000D3A20F23EA95811E7C0A95563344E</td>\n",
       "      <td>7</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.027065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000D3A20F23EA95811E7C7892A0CE261</td>\n",
       "      <td>5</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>699</td>\n",
       "      <td>200</td>\n",
       "      <td>0.183698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000D3A20F23EA95811E7CD686C396528</td>\n",
       "      <td>6</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>20</td>\n",
       "      <td>0.022180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000D3A20F23EA95911E7CEA8C574EDAE</td>\n",
       "      <td>5</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.043432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000D3A20F23EA95911E7D4F05C59C978</td>\n",
       "      <td>7</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.054016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6801</th>\n",
       "      <td>E25501F0CF189F4711ECF161D396AAEB</td>\n",
       "      <td>5</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>250</td>\n",
       "      <td>0.011764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6802</th>\n",
       "      <td>000D3AAC977BBB2F11ECDE319AE8B765</td>\n",
       "      <td>7</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>200</td>\n",
       "      <td>0.016657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>000D3A39D824A81611E922DAAA472ACF</td>\n",
       "      <td>6</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>20</td>\n",
       "      <td>0.045454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>000D3A39D824A82E11E983DB973D46C8</td>\n",
       "      <td>7</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1149</td>\n",
       "      <td>200</td>\n",
       "      <td>0.061874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>000D3A21DA51A81311EA440F30295A02</td>\n",
       "      <td>7</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>250</td>\n",
       "      <td>0.033838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6806 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ClientUUId  Id OrderType        LocalBeginDate  \\\n",
       "0     000D3A20F23EA95811E7C0A95563344E   7       2,3  2023-11-02T00:00:00Z   \n",
       "1     000D3A20F23EA95811E7C7892A0CE261   5       2,3  2023-11-02T00:00:00Z   \n",
       "2     000D3A20F23EA95811E7CD686C396528   6       2,3  2023-11-02T00:00:00Z   \n",
       "3     000D3A20F23EA95911E7CEA8C574EDAE   5       2,3  2023-11-02T00:00:00Z   \n",
       "4     000D3A20F23EA95911E7D4F05C59C978   7       2,3  2023-11-02T00:00:00Z   \n",
       "...                                ...  ..       ...                   ...   \n",
       "6801  E25501F0CF189F4711ECF161D396AAEB   5     1,2,3  2023-11-02T00:00:00Z   \n",
       "6802  000D3AAC977BBB2F11ECDE319AE8B765   7     1,2,3  2023-11-02T00:00:00Z   \n",
       "6803  000D3A39D824A81611E922DAAA472ACF   6     1,2,3  2023-11-02T00:00:00Z   \n",
       "6804  000D3A39D824A82E11E983DB973D46C8   7     1,2,3  2023-11-02T00:00:00Z   \n",
       "6805  000D3A21DA51A81311EA440F30295A02   7     1,2,3  2023-11-02T00:00:00Z   \n",
       "\n",
       "              LocalEndDate  OrderPrice  Discount  apply_promo  \n",
       "0     2023-11-05T23:59:00Z         799       200     0.027065  \n",
       "1     2023-11-05T23:59:00Z         699       200     0.183698  \n",
       "2     2023-11-05T23:59:00Z         799        20     0.022180  \n",
       "3     2023-11-05T23:59:00Z         799       200     0.043432  \n",
       "4     2023-11-05T23:59:00Z         799       200     0.054016  \n",
       "...                    ...         ...       ...          ...  \n",
       "6801  2023-11-05T23:59:00Z        1249       250     0.011764  \n",
       "6802  2023-11-05T23:59:00Z        1249       200     0.016657  \n",
       "6803  2023-11-05T23:59:00Z        1249        20     0.045454  \n",
       "6804  2023-11-05T23:59:00Z        1149       200     0.061874  \n",
       "6805  2023-11-05T23:59:00Z        1249       250     0.033838  \n",
       "\n",
       "[6806 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open('model_2.pkl', 'wb') as f:\n",
    "    pickle.dump(models_2, f)\n",
    "    \n",
    "with open('meta_model.pkl', 'wb') as f:\n",
    "    pickle.dump(meta_model, f)\n",
    "    \n",
    "with open('meta_model_2.pkl', 'wb') as f:\n",
    "    pickle.dump(meta_model_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
