{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import polars as pl\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import shap\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset():\n",
    "    df = pd.read_csv('data/train_target.csv')\n",
    "    orders = pd.read_csv('data/orders.csv')\n",
    "    events = pd.read_csv('data/mobile_events.csv')\n",
    "    promocodes = pd.read_csv('data/clients_promo_october.csv')\n",
    "\n",
    "    df.drop(['LocalBeginDate', 'LocalEndDate'], axis=1, inplace=True)\n",
    "\n",
    "    orders['SaleDate'] = pd.to_datetime(orders['SaleDate'])\n",
    "    orders['Date'] = pd.to_datetime(orders['Date'])\n",
    "\n",
    "    events['Timestamp'] = pd.to_datetime(events['Timestamp'])\n",
    "    promocodes['LocalBeginDate'] = pd.to_datetime(promocodes['LocalBeginDate'])\n",
    "    promocodes['LocalEndDate'] = pd.to_datetime(promocodes['LocalEndDate'])\n",
    "    events['Timestamp'] = events['Timestamp'].dt.floor('s')\n",
    "    \n",
    "    num_promocodes = promocodes.groupby('ClientUUId').agg({'Id': 'count'}).reset_index().rename(columns={'Id': 'num_promocodes'})\n",
    "    df_october = orders[orders.Date.dt.month == 10]\n",
    "    test_1 = df_october.groupby(['ClientUUId', 'OrderUUId']).agg(\n",
    "        {'apply_promo': 'first'}\n",
    "    ).reset_index().drop('OrderUUId', axis=1).groupby('ClientUUId') \\\n",
    "    .agg({'apply_promo': 'sum'}).reset_index().rename(columns={'apply_promo': 'apply_promo_used_last_month'})\n",
    "\n",
    "    test_1 = test_1.merge(num_promocodes, on='ClientUUId')\n",
    "    test_1['ratio_apply_promo_to_num_promocodes'] = test_1['apply_promo_used_last_month'] / test_1['num_promocodes']\n",
    "    test_1.drop(columns=['apply_promo_used_last_month', 'num_promocodes'], inplace=True)\n",
    "    df = df.merge(test_1, on='ClientUUId', how='left')\n",
    "    events['hour'] = events['Timestamp'].dt.hour\n",
    "    df_events = pl.from_pandas(events).group_by('ClientUUId').agg(\n",
    "        pl.col('VisitToken').n_unique().alias('VisitToken_n_unique'),\n",
    "        pl.col('hour').median().alias('hour_events_median'),\n",
    "        pl.col('Platform').first(),\n",
    "    ).to_pandas()\n",
    "\n",
    "    ohe = OneHotEncoder(sparse_output=False, dtype=np.int8)\n",
    "    ohe.fit(events[['EventName']])\n",
    "\n",
    "    new_features = pd.DataFrame(ohe.transform(events[['EventName']]), columns=ohe.get_feature_names_out())\n",
    "    events = pd.concat([events, new_features], axis=1)\n",
    "    events = pl.from_pandas(events)\n",
    "    df_events2 = events.groupby('ClientUUId').agg(\n",
    "        *[pl.col(col).mean().alias(f'{col}_mean') for col in ohe.get_feature_names_out()],\n",
    "        *[pl.col(col).sum().alias(f'{col}_sum') for col in ohe.get_feature_names_out()],\n",
    "        ((pl.datetime(2023, 11, 2, time_unit='ns', time_zone='UTC') - pl.col('Timestamp').last()).dt.total_seconds() / 3600).alias('last_online')\n",
    "    )\n",
    "    \n",
    "    df_events = df_events.merge(df_events2.to_pandas(), on='ClientUUId', how='left')\n",
    "    events = events.to_pandas()\n",
    "    \n",
    "    \n",
    "    df_events['Platform'] = df_events['Platform'].map({'ios': 1, 'android': 0})\n",
    "    df = df.merge(df_events, on='ClientUUId', how='left')\n",
    "    \n",
    "    id_ohe = OneHotEncoder(sparse_output=False, dtype=np.int8)\n",
    "    id_features = pd.DataFrame(id_ohe.fit_transform(promocodes[['Id']]), columns=id_ohe.get_feature_names_out())\n",
    "    promocodes = pd.concat([promocodes, id_features], axis=1)\n",
    "    promocodes['promocode_duration'] = ((promocodes['LocalEndDate'] - promocodes['LocalBeginDate']).dt.total_seconds() / 3600)\n",
    "    promocodes['OrderType'] = promocodes['OrderType'].map({'2,3': 0, '1,2,3': 1})\n",
    "    promocodes['Discount_percenteges'] = np.where(promocodes['Discount'] <= 100, promocodes['Discount'], 0)\n",
    "    promocodes['Discount_usual'] = np.where(promocodes['Discount'] > 100, promocodes['Discount'], 0)\n",
    "    promocodes = pl.from_pandas(promocodes)\n",
    "    promocodes = promocodes.groupby('ClientUUId').agg(\n",
    "        pl.col('Id').count().alias('num_promocodes'),\n",
    "        *[pl.col(col).mean().alias(f'{col}_mean') for col  in id_ohe.get_feature_names_out()],\n",
    "        *[pl.col(col).sum().alias(f'{col}_sum') for col  in id_ohe.get_feature_names_out()],\n",
    "        pl.col('OrderPrice').max().alias('OrderPrice_max'),\n",
    "        pl.col('OrderPrice').min().alias('OrderPrice_min'),\n",
    "        pl.col('OrderPrice').median().alias('OrderPrice_median'),\n",
    "        pl.col('Discount').max().alias('Discount_max'),\n",
    "        pl.col('Discount').min().alias('Discount_min'),\n",
    "        pl.col('Discount').median().alias('Discount_median'),\n",
    "    )\n",
    "    promocodes = promocodes.to_pandas()\n",
    "    df = df.merge(promocodes, on='ClientUUId', how='left')\n",
    "    \n",
    "    orders['in_restaurant'] = orders['addressId'].isnull().astype(np.int8)\n",
    "    \n",
    "    df_orders = orders.groupby('ClientUUId').agg({'NewClient': 'max', 'Date': ['max', 'min'], \n",
    "                                  'ClientOrderNumber': ['max', 'min'],\n",
    "                                #   'in_restaurant': ['sum', 'mean'], #test\n",
    "                                  }).reset_index()\n",
    "\n",
    "    df_orders.columns = ['_'.join(col).strip() for col in df_orders.columns.values]\n",
    "    df_orders.rename(columns={'ClientUUId_': 'ClientUUId'}, inplace=True)\n",
    "\n",
    "    df_orders['orders_in_this_year'] = df_orders['ClientOrderNumber_max'] - df_orders['ClientOrderNumber_min']\n",
    "    df_orders['last_order_was'] = (datetime(2023, 11, 1) - df_orders['Date_max']).dt.days \n",
    "\n",
    "    orders.drop(columns=['NewClient'], inplace=True, errors='ignore')\n",
    "\n",
    "    orders['discount_for_product'] = orders['MenuPrice'] - orders['ProductTotalPrice']\n",
    "    \n",
    "\n",
    "    enc = OneHotEncoder(sparse_output=False)\n",
    "    enc.fit(orders[['CategoryId']])\n",
    "    categories_orders  = pd.DataFrame(enc.transform(orders[['CategoryId']]), columns=enc.get_feature_names_out())\n",
    "\n",
    "    orders = pd.concat([orders, categories_orders], axis=1)\n",
    "    def map_hour_to_time_of_day(hour):\n",
    "        if 0 <= hour < 6:\n",
    "            return 'Night'\n",
    "        elif 6 <= hour <= 12:\n",
    "            return 'Morning'\n",
    "        elif 12 < hour <= 17:\n",
    "            return 'Lunch'\n",
    "        else:\n",
    "            return 'Dinner'\n",
    "        \n",
    "    orders['TimeOfDay'] = orders['SaleDate'].dt.hour.map(map_hour_to_time_of_day)\n",
    "\n",
    "    enc_time_of_day = OneHotEncoder(sparse_output=False)\n",
    "    enc_time_of_day.fit(orders[['TimeOfDay']])\n",
    "    enc_time_of_day_orders  = pd.DataFrame(enc_time_of_day.transform(orders[['TimeOfDay']]), columns=enc_time_of_day.get_feature_names_out())\n",
    "    orders = pd.concat([orders, enc_time_of_day_orders], axis=1)\n",
    "    orders['weekday'] = orders['SaleDate'].dt.weekday\n",
    "    \n",
    "    enc_weekday = OneHotEncoder(sparse_output=False)\n",
    "    enc_weekday.fit(orders[['weekday']])\n",
    "    enc_weekday_data  = pd.DataFrame(enc_weekday.transform(orders[['weekday']]), columns=enc_weekday.get_feature_names_out())\n",
    "    orders = pd.concat([orders, enc_weekday_data], axis=1)\n",
    "    \n",
    "    orders = pl.from_pandas(orders)\n",
    "    orders_agg = orders.group_by(['ClientUUId', 'OrderUUId']).agg(\n",
    "        pl.col('in_restaurant').first(),\n",
    "        pl.col('OrderTotalPrice').first(),\n",
    "        pl.col('OrderPaymentType').first(),\n",
    "        pl.col('OrderType').first(),\n",
    "        pl.col('OrderState').first(),\n",
    "        pl.col('ClientOrderNumber').first(),\n",
    "        pl.col('apply_promo').first(),\n",
    "        pl.col('Date').first(),\n",
    "        pl.col('addressId').first(),\n",
    "        pl.col('deliverySectorId').first(),\n",
    "        pl.col('SaleDate').first(),\n",
    "        pl.col('UnitUUId').first(),\n",
    "        pl.col('ProductUUId').count().alias('ProductUUId_count'),\n",
    "        pl.col('ProductUUId').unique().count().alias('ProductUUId_unique_count'),\n",
    "        pl.col('discount_for_product').mean().alias('discount_for_product_mean'),\n",
    "        pl.col('discount_for_product').sum().alias('discount_for_product_sum'),\n",
    "        pl.col('CategoryId').mode().get(0).alias('CategoryId_mode'),\n",
    "        pl.col('MenuPrice').max().alias('MenuPrice_max'),\n",
    "        pl.col('MenuPrice').min().alias('MenuPrice_min'),\n",
    "        pl.col('MenuPrice').median().alias('MenuPrice_median'),\n",
    "        pl.col('ProductTotalPrice').max().alias('ProductTotalPrice_max'),\n",
    "        pl.col('ProductTotalPrice').min().alias('ProductTotalPrice_min'),\n",
    "        pl.col('ProductTotalPrice').median().alias('ProductTotalPrice_median'),\n",
    "        *[pl.col(col).first() for col in enc_time_of_day.get_feature_names_out()],\n",
    "        \n",
    "        pl.col('ProductTotalPrice').sum().alias('ProductTotalPrice_sum'),\n",
    "        pl.col('ProductTotalPrice').mean().alias('ProductTotalPrice_mean'),\n",
    "        *[pl.col(col).first() for col in enc_weekday.get_feature_names_out()],\n",
    "        \n",
    "    )\n",
    "    orders_agg = orders_agg.sort(by=['ClientUUId', 'ClientOrderNumber'])\n",
    "\n",
    "    orders_agg = orders_agg.group_by('ClientUUId').agg(\n",
    "        pl.col('UnitUUId').unique().count().alias('num_of_UnitUUId'), \n",
    "        *[func('OrderTotalPrice').alias(f'OrderTotalPrice_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last]],\n",
    "        *[func('apply_promo').alias(f'apply_promo_{func.__name__}') for func in [pl.sum, pl.mean, pl.last]],\n",
    "        pl.last('ClientOrderNumber').alias('ClientOrderNumber_last'),\n",
    "        pl.col('deliverySectorId').unique().count().alias(f'deliverySectorId_unq_count'),\n",
    "        *[func('MenuPrice_max').alias(f'MenuPrice_max_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('MenuPrice_min').alias(f'MenuPrice_min_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('MenuPrice_median').alias(f'MenuPrice_median_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductTotalPrice_max').alias(f'ProductTotalPrice_max_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductTotalPrice_min').alias(f'ProductTotalPrice_min_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductTotalPrice_median').alias(f'ProductTotalPrice_median_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('ProductUUId_unique_count').alias(f'ProductUUId_unique_count_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.mean, pl.last, pl.sum]],\n",
    "        *[func('TimeOfDay_Dinner').alias(f'TimeOfDay_Dinner_{func.__name__}') for func in [pl.sum, pl.mean, pl.last]],\n",
    "        *[func('TimeOfDay_Lunch').alias(f'TimeOfDay_Lunch_{func.__name__}') for func in [pl.sum, pl.mean, pl.last]],\n",
    "        *[func('TimeOfDay_Morning').alias(f'TimeOfDay_Morning_{func.__name__}') for func in [pl.sum, pl.mean, pl.last]],\n",
    "        *[func('TimeOfDay_Night').alias(f'TimeOfDay_Night_{func.__name__}') for func in [pl.sum, pl.mean, pl.last]],\n",
    "        pl.col('apply_promo').filter(pl.col('Date').dt.month() == 10).mean().alias('apply_promo_mean_last_month'),\n",
    "        pl.col('apply_promo').filter(pl.col('Date').dt.month() == 10).sum().alias('apply_promo_sum_last_month'),\n",
    "        pl.col('in_restaurant').filter(pl.col('Date').dt.month() == 10).mean().alias('in_restaurant_mean_last_month'),\n",
    "        pl.col('in_restaurant').filter(pl.col('Date').dt.month() == 10).sum().alias('in_restaurant_sum_last_month'),\n",
    "        pl.col('in_restaurant').mean().alias('in_restaurant_mean'),\n",
    "        pl.col('in_restaurant').sum().alias('in_restaurant_sum'),\n",
    "        \n",
    "        *[func('ProductTotalPrice_sum').alias(f'ProductTotalPrice_sum_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.sum]],\n",
    "        *[func('ProductTotalPrice_mean').alias(f'ProductTotalPrice_mean_{func.__name__}') for func in [pl.max, pl.min, pl.median, pl.sum]],\n",
    "    )\n",
    "    \n",
    "    df_orders = df_orders.merge(orders_agg.to_pandas(), on='ClientUUId', how='left')\n",
    "    df = df.merge(df_orders, on='ClientUUId', how='left')\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    df['groups'] = le.fit_transform(df[['ClientUUId']])\n",
    "    test = pd.read_csv('data/test-2.csv')\n",
    "    \n",
    "    promocodes_copy = pd.read_csv('data/clients_promo_october.csv')\n",
    "    promocodes_copy['LocalBeginDate'] = pd.to_datetime(promocodes_copy['LocalBeginDate']).dt.tz_convert(None)\n",
    "    promocodes_copy['LocalEndDate'] = pd.to_datetime(promocodes_copy['LocalEndDate']).dt.tz_convert(None)\n",
    "    orders_agg = orders.group_by(['ClientUUId', 'OrderUUId']).agg(\n",
    "        pl.col('apply_promo').first(),\n",
    "        pl.col('Date').first(),\n",
    "        pl.col('OrderTotalPrice').first(),\n",
    "        pl.col('OrderType').first(),\n",
    "    ).filter((pl.col('apply_promo') == 1) & (pl.col('Date').dt.month() == 10))\n",
    "    orders_agg = orders_agg.join(pl.from_pandas(promocodes_copy), on='ClientUUId', how='left')\n",
    "    orders_agg = orders_agg.filter((pl.col('OrderTotalPrice')>=pl.col('OrderPrice')) & (pl.col('LocalBeginDate') <= pl.col('Date')) \n",
    "                    & (pl.col('Date') <= pl.col('LocalEndDate')) & (pl.col('OrderType_right').str.find(pl.col('OrderType')) > -1)).to_pandas()\n",
    "\n",
    "    idx = orders_agg.groupby(['ClientUUId', 'Date'])['OrderPrice'].idxmax()\n",
    "    orders_agg = orders_agg.loc[idx].sort_values(by='ClientUUId')\n",
    "    orders_agg.sort_values(by=['ClientUUId', 'Date'])\n",
    "    orders_agg = pl.from_pandas(orders_agg)\n",
    "    orders_agg = orders_agg.group_by(['ClientUUId']).agg(\n",
    "        pl.col('OrderTotalPrice').last().alias('OrderTotalPrice_last_fp'),\n",
    "        pl.col('OrderType').last().alias('OrderType_last_fp'),\n",
    "        pl.col('Id').last().alias('Id_last_fp'),\n",
    "        pl.col('OrderPrice').last().alias('OrderPrice_last_fp'),\n",
    "        pl.col('Discount').last().alias('Discount_last_fp'),\n",
    "    ).to_pandas()\n",
    "    df = df.merge(orders_agg , on='ClientUUId', how='left')\n",
    "    columns = ['OrderTotalPrice_last_fp', 'OrderType_last_fp', 'Id_last_fp', 'OrderPrice_last_fp', 'Discount_last_fp']\n",
    "    df[columns] = df[columns].fillna(-1)\n",
    "    promocodes_equality = df[df.apply(lambda row: row['OrderType'].find(str(row['OrderType_last_fp'])) > -1 \n",
    "                                      and row['Id'] == row['Id_last_fp'], axis=1)][['ClientUUId', 'Id', 'OrderType']]\n",
    "    promocodes_equality['promocodes_equality'] = 1\n",
    "    df = df.merge(promocodes_equality, on=['ClientUUId', 'Id', 'OrderType'], how='left')\n",
    "    df['promocodes_equality'] = df['promocodes_equality'].fillna(0)\n",
    "\n",
    "    test = test.merge(orders_agg , on='ClientUUId', how='left')\n",
    "    columns = ['OrderTotalPrice_last_fp', 'OrderType_last_fp', 'Id_last_fp', 'OrderPrice_last_fp', 'Discount_last_fp']\n",
    "    test[columns] = test[columns].fillna(-1)\n",
    "    test = test.merge(promocodes_equality, on=['ClientUUId', 'Id', 'OrderType'], how='left')\n",
    "    test['promocodes_equality'] = test['promocodes_equality'].fillna(0)\n",
    "    \n",
    "    test = test.merge(df_orders, on='ClientUUId', how='left')\n",
    "    test = test.merge(df_events, on='ClientUUId', how='left')\n",
    "    test = test.merge(promocodes, on='ClientUUId', how='left')\n",
    "    test = test.merge(test_1, on='ClientUUId', how='left')\n",
    "    \n",
    "    df.drop(columns, inplace=True, axis=1)\n",
    "    test.drop(columns, inplace=True, axis=1)\n",
    "    \n",
    "    ohe = OneHotEncoder(sparse_output=False, dtype=np.int8)\n",
    "    ohe.fit(df[['Id']])\n",
    "\n",
    "    new_features = pd.DataFrame(ohe.transform(df[['Id']]), columns=ohe.get_feature_names_out())\n",
    "    df = pd.concat([df, new_features], axis=1)\n",
    "    # df.drop(columns=['Id'], inplace=True)\n",
    "    \n",
    "    new_features = pd.DataFrame(ohe.transform(test[['Id']]), columns=ohe.get_feature_names_out())\n",
    "    test = pd.concat([test, new_features], axis=1)\n",
    "    # test.drop(columns=['Id'], inplace=True)\n",
    "    \n",
    "    test['Is_id_7_mean'] = test['Id_7_mean'] * test['Id_7']\n",
    "    test['Is_id_7_sum'] = test['Id_7_sum'] * test['Id_7']\n",
    "\n",
    "    test['Is_id_6_mean'] = test['Id_6_mean'] * test['Id_6']\n",
    "    test['Is_id_6_sum'] = test['Id_6_sum'] * test['Id_6']\n",
    "\n",
    "    test['Is_id_5_mean'] = test['Id_5_mean'] * test['Id_5']\n",
    "    test['Is_id_5_sum'] = test['Id_5_sum'] * test['Id_5']\n",
    "\n",
    "    df['Is_id_7_mean'] = df['Id_7_mean'] * df['Id_7']\n",
    "    df['Is_id_7_sum'] = df['Id_7_sum'] * df['Id_7']\n",
    "\n",
    "    df['Is_id_6_mean'] = df['Id_6_mean'] * df['Id_6']\n",
    "    df['Is_id_6_sum'] = df['Id_6_sum'] * df['Id_6']\n",
    "\n",
    "    df['Is_id_5_mean'] = df['Id_5_mean'] * df['Id_5']\n",
    "    df['Is_id_5_sum'] = df['Id_5_sum'] * df['Id_5']\n",
    "    return df, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/jh5364154tl5pr4qr45tg4880000gn/T/ipykernel_70528/1607614134.py:42: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  df_events2 = events.groupby('ClientUUId').agg(\n",
      "/var/folders/px/jh5364154tl5pr4qr45tg4880000gn/T/ipykernel_70528/1607614134.py:70: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  promocodes = promocodes.groupby('ClientUUId').agg(\n",
      "/usr/local/lib/python3.10/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "df, test = prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27741, 134),\n",
       " ['ClientOrderNumber_min',\n",
       "  'apply_promo_mean',\n",
       "  'ClientOrderNumber_last',\n",
       "  'TimeOfDay_Morning_mean',\n",
       "  'TimeOfDay_Night_mean',\n",
       "  'in_restaurant_sum',\n",
       "  'ProductTotalPrice_sum_max',\n",
       "  'ProductTotalPrice_sum_min',\n",
       "  'ProductTotalPrice_sum_median'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_high_correlation_features(df):\n",
    "    corr_matrix = df.select_dtypes([float, int]).corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.99)]\n",
    "    return to_drop\n",
    "to_drop = find_high_correlation_features(df)\n",
    "df.drop(columns=to_drop, inplace=True)\n",
    "test.drop(columns=to_drop, inplace=True)\n",
    "\n",
    "df.shape, to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_for_model_1 = df.dropna(axis=0).index\n",
    "X = df.iloc[index_for_model_1].select_dtypes([float, int]).drop(columns=['apply_promo', 'groups'])\n",
    "y = df.iloc[index_for_model_1]['apply_promo']\n",
    "groups = df.iloc[index_for_model_1]['groups']\n",
    "result_df = pd.DataFrame({'score': 0.0, 'ClientUUId': df['ClientUUId'], 'OrderType' : df['OrderType'], 'target': df['apply_promo']\n",
    "                          }, index=df.iloc[index_for_model_1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "[0]\ttrain-auc:0.73393\teval-auc:0.73051\n",
      "[50]\ttrain-auc:0.82672\teval-auc:0.76508\n",
      "[100]\ttrain-auc:0.85918\teval-auc:0.75886\n",
      "[112]\ttrain-auc:0.86427\teval-auc:0.75957\n",
      "Fold 2:\n",
      "[0]\ttrain-auc:0.72140\teval-auc:0.74158\n",
      "[50]\ttrain-auc:0.81342\teval-auc:0.76935\n",
      "[100]\ttrain-auc:0.84672\teval-auc:0.77873\n",
      "[111]\ttrain-auc:0.85284\teval-auc:0.77892\n",
      "Fold 3:\n",
      "[0]\ttrain-auc:0.72760\teval-auc:0.70785\n",
      "[50]\ttrain-auc:0.82083\teval-auc:0.76138\n",
      "[100]\ttrain-auc:0.85300\teval-auc:0.77029\n",
      "[150]\ttrain-auc:0.86998\teval-auc:0.76473\n",
      "[175]\ttrain-auc:0.87478\teval-auc:0.76034\n",
      "Fold 4:\n",
      "[0]\ttrain-auc:0.74380\teval-auc:0.62236\n",
      "[50]\ttrain-auc:0.82617\teval-auc:0.71169\n",
      "[100]\ttrain-auc:0.85470\teval-auc:0.70730\n",
      "[140]\ttrain-auc:0.86479\teval-auc:0.70980\n",
      "Fold 5:\n",
      "[0]\ttrain-auc:0.75255\teval-auc:0.70234\n",
      "[50]\ttrain-auc:0.82254\teval-auc:0.75357\n",
      "[100]\ttrain-auc:0.84757\teval-auc:0.76931\n",
      "[150]\ttrain-auc:0.86443\teval-auc:0.78269\n",
      "[200]\ttrain-auc:0.87286\teval-auc:0.78358\n",
      "[250]\ttrain-auc:0.87778\teval-auc:0.78150\n",
      "[272]\ttrain-auc:0.87855\teval-auc:0.78115\n",
      "Fold 6:\n",
      "[0]\ttrain-auc:0.72721\teval-auc:0.70111\n",
      "[50]\ttrain-auc:0.81882\teval-auc:0.75908\n",
      "[100]\ttrain-auc:0.85582\teval-auc:0.76156\n",
      "[150]\ttrain-auc:0.87195\teval-auc:0.76886\n",
      "[200]\ttrain-auc:0.87984\teval-auc:0.77035\n",
      "[250]\ttrain-auc:0.88466\teval-auc:0.77276\n",
      "[300]\ttrain-auc:0.88752\teval-auc:0.77160\n",
      "[350]\ttrain-auc:0.89193\teval-auc:0.77217\n",
      "[400]\ttrain-auc:0.89333\teval-auc:0.77079\n",
      "[422]\ttrain-auc:0.89386\teval-auc:0.77066\n",
      "Fold 7:\n",
      "[0]\ttrain-auc:0.74286\teval-auc:0.76454\n",
      "[50]\ttrain-auc:0.82645\teval-auc:0.78439\n",
      "[100]\ttrain-auc:0.85263\teval-auc:0.78772\n",
      "[102]\ttrain-auc:0.85369\teval-auc:0.79012\n",
      "Fold 8:\n",
      "[0]\ttrain-auc:0.75876\teval-auc:0.80258\n",
      "[50]\ttrain-auc:0.81497\teval-auc:0.82878\n",
      "[100]\ttrain-auc:0.84603\teval-auc:0.84361\n",
      "[150]\ttrain-auc:0.86416\teval-auc:0.84973\n",
      "[200]\ttrain-auc:0.87108\teval-auc:0.85235\n",
      "[250]\ttrain-auc:0.87587\teval-auc:0.85332\n",
      "[300]\ttrain-auc:0.88048\teval-auc:0.85307\n",
      "[326]\ttrain-auc:0.88072\teval-auc:0.85312\n",
      "Fold 9:\n",
      "[0]\ttrain-auc:0.73417\teval-auc:0.74209\n",
      "[50]\ttrain-auc:0.82879\teval-auc:0.74135\n",
      "[100]\ttrain-auc:0.85628\teval-auc:0.75330\n",
      "[150]\ttrain-auc:0.86842\teval-auc:0.75563\n",
      "[200]\ttrain-auc:0.87287\teval-auc:0.76086\n",
      "[250]\ttrain-auc:0.87654\teval-auc:0.76042\n",
      "[290]\ttrain-auc:0.88071\teval-auc:0.75790\n",
      "Fold 10:\n",
      "[0]\ttrain-auc:0.72762\teval-auc:0.74502\n",
      "[50]\ttrain-auc:0.82426\teval-auc:0.78013\n",
      "[100]\ttrain-auc:0.85562\teval-auc:0.77841\n",
      "[113]\ttrain-auc:0.85949\teval-auc:0.77783\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "num_iter = []\n",
    "skf = StratifiedGroupKFold(n_splits=10)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y, groups)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index])\n",
    "    dtest = xgb.DMatrix(X.iloc[test_index], label=y.iloc[test_index])\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"n_estimators\": 3000,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "        'max_depth': 5,\n",
    "        'colsample_bytree': 0.6489131779549984,\n",
    "        'colsample_bynode': 0.6009610046445969,\n",
    "        'colsample_bylevel': 0.7957273275573362,\n",
    "        'subsample': 0.6805161691122295,\n",
    "        'alpha': 8.371930284576981,\n",
    "        'lambda': 5.649133354153159,\n",
    "        'gamma': 7.103762168902399,\n",
    "        'min_child_weight': 4.101573431623777,\n",
    "        'max_delta_step': 1,\n",
    "        'sampling_method': 'uniform'\n",
    " }\n",
    "\n",
    "    evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=90,\n",
    "        verbose_eval=50,\n",
    "    )\n",
    "\n",
    "    result_df.iloc[test_index, 0] = model.predict(dtest)\n",
    "    models.append(model)\n",
    "    num_iter.append(model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7729569554083207"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(result_df['target'].astype(float), result_df['score'].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model without orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27741, 48)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=1,thresh=18_000)\n",
    "\n",
    "X = df.select_dtypes([float, int]).drop(columns=['apply_promo', 'groups'])\n",
    "y = df['apply_promo']\n",
    "groups = df['groups']\n",
    "result_df2 = pd.DataFrame({'score': 0.0, 'ClientUUId': df['ClientUUId'], 'OrderType' : df['OrderType'], 'target': df['apply_promo']\n",
    "                          }, index=df.index)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.77843\teval-auc:0.74794\n",
      "[200]\ttrain-auc:0.79319\teval-auc:0.75513\n",
      "[279]\ttrain-auc:0.79637\teval-auc:0.75420\n",
      "Fold 2:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.79186\teval-auc:0.68994\n",
      "[158]\ttrain-auc:0.79949\teval-auc:0.68972\n",
      "Fold 3:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.78336\teval-auc:0.77062\n",
      "[200]\ttrain-auc:0.79542\teval-auc:0.77681\n",
      "[300]\ttrain-auc:0.79811\teval-auc:0.77671\n",
      "[400]\ttrain-auc:0.80028\teval-auc:0.77663\n",
      "[419]\ttrain-auc:0.80030\teval-auc:0.77660\n",
      "Fold 4:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.78802\teval-auc:0.74143\n",
      "[157]\ttrain-auc:0.79667\teval-auc:0.73970\n",
      "Fold 5:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.78832\teval-auc:0.68448\n",
      "[200]\ttrain-auc:0.80093\teval-auc:0.69618\n",
      "[300]\ttrain-auc:0.80282\teval-auc:0.69690\n",
      "[371]\ttrain-auc:0.80438\teval-auc:0.69696\n",
      "Fold 6:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.78391\teval-auc:0.77676\n",
      "[200]\ttrain-auc:0.79587\teval-auc:0.78131\n",
      "[266]\ttrain-auc:0.79712\teval-auc:0.77829\n",
      "Fold 7:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.79176\teval-auc:0.69806\n",
      "[200]\ttrain-auc:0.80116\teval-auc:0.70230\n",
      "[300]\ttrain-auc:0.80319\teval-auc:0.70466\n",
      "[385]\ttrain-auc:0.80360\teval-auc:0.70464\n",
      "Fold 8:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.78832\teval-auc:0.77895\n",
      "[200]\ttrain-auc:0.79563\teval-auc:0.78546\n",
      "[254]\ttrain-auc:0.79722\teval-auc:0.78421\n",
      "Fold 9:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[92]\ttrain-auc:0.79862\teval-auc:0.63097\n",
      "Fold 10:\n",
      "[0]\ttrain-auc:0.50000\teval-auc:0.50000\n",
      "[100]\ttrain-auc:0.78848\teval-auc:0.71993\n",
      "[200]\ttrain-auc:0.80047\teval-auc:0.72654\n",
      "[300]\ttrain-auc:0.80587\teval-auc:0.73116\n",
      "[400]\ttrain-auc:0.80599\teval-auc:0.73129\n",
      "[453]\ttrain-auc:0.80655\teval-auc:0.73029\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found `n_estimators` in params. Will use it instead of argument\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found `num_boost_round` in params. Will use it instead of argument\")\n",
    "\n",
    "models_2 = []\n",
    "num_iter_2 = []\n",
    "skf = StratifiedGroupKFold(n_splits=10)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y, groups)):\n",
    "    print(f\"Fold {i + 1}:\")\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X.iloc[train_index], label=y.iloc[train_index])\n",
    "    dtest = xgb.DMatrix(X.iloc[test_index], label=y.iloc[test_index])\n",
    "    \n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"verbosity\": 0,\n",
    "        'max_depth': 4,\n",
    "        'colsample_bytree': 0.7055286391212441,\n",
    "        'colsample_bynode': 0.6915897982585256,\n",
    "        'colsample_bylevel': 0.7855187905031771,\n",
    "        'subsample': 0.7684073221158629,\n",
    "        'alpha': 5.250995916757242,\n",
    "        'lambda': 0.7316373694506648,\n",
    "        'gamma': 7.108685845995447,\n",
    "        'min_child_weight': 6.354648841169535,\n",
    "        'max_delta_step': 0,\n",
    " }\n",
    "        \n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=watchlist,\n",
    "        early_stopping_rounds=90,\n",
    "        verbose_eval=100,\n",
    "    )\n",
    "\n",
    "    result_df2.iloc[test_index, 0] = model.predict(dtest)\n",
    "    models_2.append(model)\n",
    "    num_iter_2.append(model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(columns=result_df2.columns)\n",
    "for row in result_df2.iterrows():\n",
    "    if row[0] in result_df.index:\n",
    "        result.loc[row[0], :] = result_df.loc[row[0]]\n",
    "    else:\n",
    "        result.loc[row[0], :] = result_df2.iloc[row[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7680729942000759"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(result['target'].astype(float), result['score'].astype(float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.select_dtypes([float, int])\n",
    "test_result = pd.read_csv('data/test-2.csv')\n",
    "test_result['apply_promo'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6806/6806 [02:02<00:00, 55.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "final_scores = []\n",
    "\n",
    "for i in tqdm(range(len(test))):\n",
    "    scores = []\n",
    "    data = test.iloc[i]\n",
    "    if data.isnull().sum() > 40:\n",
    "        data = pd.DataFrame(data.to_frame().T)\n",
    "        data = data[models_2[0].feature_names]\n",
    "        data_dmatrix = xgb.DMatrix(data)\n",
    "        for model in models_2:\n",
    "            scores.append(model.predict(data_dmatrix))\n",
    "    else:\n",
    "        data = pd.DataFrame(data.to_frame().T)\n",
    "        data = data[models[0].feature_names]\n",
    "        data_dmatrix = xgb.DMatrix(data)\n",
    "        for model in models:\n",
    "            scores.append(model.predict(data_dmatrix))\n",
    "            \n",
    "    final_score = np.mean(scores)\n",
    "    final_scores.append(final_score)\n",
    "\n",
    "test_result.iloc[:, -1] = final_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result.to_csv('sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClientUUId</th>\n",
       "      <th>Id</th>\n",
       "      <th>OrderType</th>\n",
       "      <th>LocalBeginDate</th>\n",
       "      <th>LocalEndDate</th>\n",
       "      <th>OrderPrice</th>\n",
       "      <th>Discount</th>\n",
       "      <th>apply_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000D3A20F23EA95811E7C0A95563344E</td>\n",
       "      <td>7</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.025063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000D3A20F23EA95811E7C7892A0CE261</td>\n",
       "      <td>5</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>699</td>\n",
       "      <td>200</td>\n",
       "      <td>0.194197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000D3A20F23EA95811E7CD686C396528</td>\n",
       "      <td>6</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>20</td>\n",
       "      <td>0.021102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000D3A20F23EA95911E7CEA8C574EDAE</td>\n",
       "      <td>5</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.053722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000D3A20F23EA95911E7D4F05C59C978</td>\n",
       "      <td>7</td>\n",
       "      <td>2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>799</td>\n",
       "      <td>200</td>\n",
       "      <td>0.059584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6801</th>\n",
       "      <td>E25501F0CF189F4711ECF161D396AAEB</td>\n",
       "      <td>5</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>250</td>\n",
       "      <td>0.012778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6802</th>\n",
       "      <td>000D3AAC977BBB2F11ECDE319AE8B765</td>\n",
       "      <td>7</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>200</td>\n",
       "      <td>0.016563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>000D3A39D824A81611E922DAAA472ACF</td>\n",
       "      <td>6</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>20</td>\n",
       "      <td>0.037174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>000D3A39D824A82E11E983DB973D46C8</td>\n",
       "      <td>7</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1149</td>\n",
       "      <td>200</td>\n",
       "      <td>0.079323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>000D3A21DA51A81311EA440F30295A02</td>\n",
       "      <td>7</td>\n",
       "      <td>1,2,3</td>\n",
       "      <td>2023-11-02T00:00:00Z</td>\n",
       "      <td>2023-11-05T23:59:00Z</td>\n",
       "      <td>1249</td>\n",
       "      <td>250</td>\n",
       "      <td>0.030031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6806 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ClientUUId  Id OrderType        LocalBeginDate  \\\n",
       "0     000D3A20F23EA95811E7C0A95563344E   7       2,3  2023-11-02T00:00:00Z   \n",
       "1     000D3A20F23EA95811E7C7892A0CE261   5       2,3  2023-11-02T00:00:00Z   \n",
       "2     000D3A20F23EA95811E7CD686C396528   6       2,3  2023-11-02T00:00:00Z   \n",
       "3     000D3A20F23EA95911E7CEA8C574EDAE   5       2,3  2023-11-02T00:00:00Z   \n",
       "4     000D3A20F23EA95911E7D4F05C59C978   7       2,3  2023-11-02T00:00:00Z   \n",
       "...                                ...  ..       ...                   ...   \n",
       "6801  E25501F0CF189F4711ECF161D396AAEB   5     1,2,3  2023-11-02T00:00:00Z   \n",
       "6802  000D3AAC977BBB2F11ECDE319AE8B765   7     1,2,3  2023-11-02T00:00:00Z   \n",
       "6803  000D3A39D824A81611E922DAAA472ACF   6     1,2,3  2023-11-02T00:00:00Z   \n",
       "6804  000D3A39D824A82E11E983DB973D46C8   7     1,2,3  2023-11-02T00:00:00Z   \n",
       "6805  000D3A21DA51A81311EA440F30295A02   7     1,2,3  2023-11-02T00:00:00Z   \n",
       "\n",
       "              LocalEndDate  OrderPrice  Discount  apply_promo  \n",
       "0     2023-11-05T23:59:00Z         799       200     0.025063  \n",
       "1     2023-11-05T23:59:00Z         699       200     0.194197  \n",
       "2     2023-11-05T23:59:00Z         799        20     0.021102  \n",
       "3     2023-11-05T23:59:00Z         799       200     0.053722  \n",
       "4     2023-11-05T23:59:00Z         799       200     0.059584  \n",
       "...                    ...         ...       ...          ...  \n",
       "6801  2023-11-05T23:59:00Z        1249       250     0.012778  \n",
       "6802  2023-11-05T23:59:00Z        1249       200     0.016563  \n",
       "6803  2023-11-05T23:59:00Z        1249        20     0.037174  \n",
       "6804  2023-11-05T23:59:00Z        1149       200     0.079323  \n",
       "6805  2023-11-05T23:59:00Z        1249       250     0.030031  \n",
       "\n",
       "[6806 rows x 8 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "    \n",
    "with open('model_2.pkl', 'wb') as f:\n",
    "    pickle.dump(models_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
